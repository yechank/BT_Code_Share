{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "#ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from helper_functions import determine_type_of_feature\n",
    "\n",
    "# Given with df and test_size(porportion) as arguments, split the\n",
    "# original df into two parts by randomly selecting indices \n",
    "def train_test_split(df, test_size):\n",
    "    test_size = int(len(df) * test_size)\n",
    "\n",
    "    # indices contain all the index values of the DF\n",
    "    indices = df.index.tolist()\n",
    "\n",
    "    # Chooses k unique random elements from a population sequence or set for test indices \n",
    "    test_indices = random.sample(population=indices, k = test_size)\n",
    "\n",
    "    # loc[] allows us to acces certain rows\n",
    "    test_df = df.loc[test_indices]\n",
    "\n",
    "    # Create test_df by dropping test_indices(rows) from original DF\n",
    "    train_df = df.drop(test_indices)\n",
    "\n",
    "    return train_df, test_df\n",
    "    \n",
    "#=================================================================================\n",
    "\n",
    "# returnign boolean value\n",
    "# True if the data is pure, False otherwise\n",
    "\n",
    "def check_purity(data):\n",
    "    # Last Column All rows\n",
    "    label_col = data[:,-1]\n",
    "    # Checking unique values in the column\n",
    "    unique_classes = np.unique(label_col)\n",
    "\n",
    "    # If only single class is in the data, return True\n",
    "    if len(unique_classes)==1:\n",
    "        return True\n",
    "    else:\n",
    "        return False    \n",
    "\n",
    "\n",
    "#=================================================================================\n",
    "\n",
    "# classifying the class type (in string data type)of given data\n",
    "# can customize the classification if the number of data point is less the k\n",
    "\n",
    "def create_leaf(data, ml_task):\n",
    "    \n",
    "    label_col = data[:,-1]\n",
    "    \n",
    "    if ml_task == \"regression\":\n",
    "        leaf = np.mean(label_col)\n",
    "        \n",
    "    #classification    \n",
    "    else:    \n",
    "        # Returning unique values with their individual counts \n",
    "        unique_classes, classes_count = np.unique(label_col,return_counts=True)\n",
    "\n",
    "        # Index of largest class\n",
    "        largest_class_index = np.argmax(classes_count)\n",
    "\n",
    "        leaf = unique_classes[largest_class_index]\n",
    "        # returning largest class    \n",
    "    \n",
    "    return leaf\n",
    "\n",
    "#=================================================================================\n",
    "\n",
    "# returning dictionary where the keys are indices of columns and \n",
    "# values are list of all the potential split\n",
    "\n",
    "# Loop only the features in random_subspace\n",
    "def get_potential_splits(data,random_subspace):\n",
    "    \n",
    "    potential_splits = {}\n",
    "    _, n_columns = data.shape\n",
    "    \n",
    "    # exclude last column of data frame(label)\n",
    "    \n",
    "    column_indices = list(range(n_columns - 1))\n",
    "    \n",
    "    # if random subspace is defined \n",
    "    if random_subspace != None and random_subspace <= len(column_indices):\n",
    "        column_indices = random.sample(population = column_indices, k = random_subspace)\n",
    "    \n",
    "    for column_index in column_indices:\n",
    "        \n",
    "        #select all the rows of particular column\n",
    "        values = data[:, column_index]\n",
    "        \n",
    "        # getting unique values from list of values\n",
    "        unique_values = np.unique(values)\n",
    "        \n",
    "        potential_splits[column_index] = unique_values\n",
    "    \n",
    "    return potential_splits\n",
    "\n",
    "#=================================================================================\n",
    "\n",
    "# Split data into sections\n",
    "# Split is determine by name of the feature(column) and numerical value\n",
    "# Returning two numpy 2D array: below and above the numerical split point \\\n",
    "\n",
    "\n",
    "def split_data(data, split_column, split_value):\n",
    "   \n",
    "    # creating mask for the split\n",
    "    split_column_values = data[:, split_column]\n",
    "\n",
    "    type_of_feature = FEATURE_TYPES[split_column]\n",
    "    \n",
    "    if type_of_feature == \"continuous\":\n",
    "        # getting only the rows(data) satisfying the condition\n",
    "        data_below = data[split_column_values <= split_value]\n",
    "        data_above = data[split_column_values >  split_value]\n",
    "     \n",
    "    else:\n",
    "        data_below = data[split_column_values == split_value]\n",
    "        data_above = data[split_column_values != split_value]\n",
    "    \n",
    "    return data_below, data_above\n",
    "\n",
    "#=================================================================================\n",
    "\n",
    "# For the regression task, we use MSE instead of entropy,\n",
    "# it calculate the deviation of actual value from the mean \n",
    "\n",
    "def calculate_mse(data):\n",
    "    actual_values = data[:, -1]\n",
    "    \n",
    "    # get_potential_split might return empty list(empty data)\n",
    "    # the if case handle the case\n",
    "    if len(actual_values) == 0:  \n",
    "        mse = 0\n",
    "        \n",
    "    else:\n",
    "        prediction = np.mean(actual_values)\n",
    "        mse = np.mean((actual_values - prediction) **2)\n",
    "    \n",
    "    return mse\n",
    "\n",
    "#=================================================================================\n",
    "\n",
    "# Iterate through all the splits from potential split function and \n",
    "# return a split with lowest overall entropy\n",
    "# -> need to calculate Entropy first follow by overall entropy\n",
    "\n",
    "# calculate entropy \n",
    "def calculate_entropy(data):\n",
    "    \n",
    "    label_col = data[:,-1]\n",
    "    # to get Pi(prob of class i), need to get the names of unique class and their count \n",
    "    _, counts = np.unique(label_col, return_counts=True)\n",
    "\n",
    "    # probabilities of each class \n",
    "    probabilities = counts / counts.sum()\n",
    "\n",
    "    # Calculating Entropy using element wise operation of numpy\n",
    "    entropy = sum(probabilities * -np.log2(probabilities))\n",
    "    \n",
    "    return entropy\n",
    "#=================================================================================\n",
    "\n",
    "# calculate entropy of entire plot\n",
    "def calculate_overall_metric(data_below, data_above, metric_function):\n",
    "\n",
    "    # based on the equation above\n",
    "    num_total_data_points = len(data_below) + len(data_above)\n",
    "\n",
    "    p_data_below = len(data_below) / num_total_data_points\n",
    "    p_data_above = len(data_above) / num_total_data_points\n",
    "\n",
    "    overall_metric = ((p_data_below * metric_function(data_below)) +\n",
    "                      (p_data_above * metric_function(data_above)))\n",
    "\n",
    "    return overall_metric\n",
    "    \n",
    "\n",
    "#=================================================================================\n",
    "\n",
    "# determine best split using data and potential split dictionary\n",
    "\n",
    "def find_best_split(data, potential_splits, ml_task):\n",
    "    '''\n",
    "    returning best_split_col, best_split_val which will be use to split the data\n",
    "    best_split_col: feature(column) resulting lowest entropy value\n",
    "    best_split_val: splitting value resulting lowest entropy value \n",
    "    '''\n",
    "    # loop over all potential splits and calculate the overall entropy of\n",
    "    # that particular split to find the parameters of best_split_col and\n",
    "    # best_split_val which generate the lowest overall entropy\n",
    "\n",
    "    first_iteration = True\n",
    "    \n",
    "    # loop over feature(column)\n",
    "    for column_index in potential_splits:\n",
    "        \n",
    "        # loop over potential split value\n",
    "        for value in potential_splits[column_index]: \n",
    "            #getting data_below and data_above for entropy calculation\n",
    "            data_below, data_above = split_data(data, \n",
    "                                                split_column = column_index, \n",
    "                                                split_value= value)\n",
    "    \n",
    "            if ml_task == \"regression\":\n",
    "                current_overall_metric = calculate_overall_metric(data_below,data_above,calculate_mse)\n",
    "\n",
    "            #calculate local overall entropy\n",
    "            else:\n",
    "                \n",
    "                current_overall_metric = calculate_overall_metric(data_below,data_above,calculate_entropy)\n",
    "\n",
    "            # check if the current_overall_entropy is smaller than currently stored entropy value\n",
    "            # in case if \n",
    "            if first_iteration == True or current_overall_metric <= overall_metric:\n",
    "                first_iteration = False\n",
    "                \n",
    "                overall_metric = current_overall_metric\n",
    "\n",
    "                # update split column and value\n",
    "                best_split_column = column_index\n",
    "                best_split_value = value\n",
    "\n",
    "        \n",
    "            \n",
    "    return best_split_column, best_split_value\n",
    "\n",
    "\n",
    "#=================================================================================\n",
    "\n",
    "\n",
    "# String data = categorical date\n",
    "def determine_data_type(df):\n",
    "    data_types = [] \n",
    "   \n",
    "    # number to decide whether the feature is categorical or continous\n",
    "    n_unique_values_threshold = 15\n",
    "    \n",
    "    for column in df.columns:\n",
    "        unique_values = df[column].unique()\n",
    "        \n",
    "        #storing an example data to check whether it is a String type data\n",
    "        example_value = unique_values[0]\n",
    "        \n",
    "        if(isinstance(example_value,str)) or (len(unique_values) <= n_unique_values_threshold):\n",
    "            data_types.append(\"categorical\")\n",
    "        else:\n",
    "            data_types.append(\"continuous\")\n",
    "        \n",
    "    return data_types\n",
    "\n",
    "\n",
    "#=================================================================================\n",
    "\n",
    "# recursively building sub tree + pruning tree functions\n",
    "# ml_task decide regresion or classification\n",
    "def decision_tree_algorithm(df, ml_task, counter = 0, min_sample = 5, max_depth = 3, random_subspace = None):\n",
    "    '''\n",
    "    df: Pandas dataframe \n",
    "    counter = counts the number of recursive call\n",
    "    min_sample: min number of sample(data) to split  \n",
    "    max_depth: max depth of the tree\n",
    "    '''\n",
    "    #for data preparation\n",
    "    if counter == 0:\n",
    "        \n",
    "        global COLUMN_HEADERS, FEATURE_TYPES\n",
    "        COLUMN_HEADERS = df.columns\n",
    "        FEATURE_TYPES = determine_data_type(df)\n",
    "        data = df.values\n",
    "    else:\n",
    "        data = df\n",
    "        \n",
    "    #base cases: if data is pure or number of data is less than min_sample\n",
    "    if (check_purity(data) == True) or (len(data) < min_sample) or (counter == max_depth):\n",
    "        \n",
    "        #return classification\n",
    "        leaf = create_leaf(data, ml_task)\n",
    "        return leaf\n",
    "    \n",
    "    # if data is not pure, begin recursive call\n",
    "    else:\n",
    "        # since it is not the first call anymore\n",
    "        counter += 1\n",
    "        \n",
    "        #using helper funtion\n",
    "        potential_splits = get_potential_splits(data,random_subspace)\n",
    "        \n",
    "        split_column, split_value = find_best_split(data, potential_splits,ml_task)\n",
    "        data_below, data_above = split_data(data, split_column, split_value)\n",
    "        \n",
    "        \n",
    "        if len(data_below) == 0 or len(data_above) == 0:\n",
    "            leaf = create_leaf(data,ml_task)\n",
    "            return leaf \n",
    "        \n",
    "        # Whole tree is made up with subtrees\n",
    "        # subtree format: {question : [In case True, In case False]}\n",
    "        \n",
    "        #instantiate subtree considering the data type\n",
    "        \n",
    "        type_of_feature = FEATURE_TYPES[split_column]\n",
    "        feature_name = COLUMN_HEADERS[split_column]\n",
    "        \n",
    "        if type_of_feature == \"continuous\":        \n",
    "            question = \"{} <= {}\".format(COLUMN_HEADERS[split_column], split_value)\n",
    "        \n",
    "        else:\n",
    "            question = \"{} == {}\".format(COLUMN_HEADERS[split_column], split_value)\n",
    "        \n",
    "        sub_tree = {question: []}\n",
    "        \n",
    "        #find True & False case : recursion \n",
    "        true_case = decision_tree_algorithm(data_below,ml_task,counter,min_sample,max_depth,random_subspace)\n",
    "        false_case = decision_tree_algorithm(data_above,ml_task,counter,min_sample,max_depth,random_subspace)\n",
    "        \n",
    "        # in case max_detph is reached and the true case and false case return the same result\n",
    "        \n",
    "        if true_case == false_case:\n",
    "            sub_tree = true_case\n",
    "        else:\n",
    "            sub_tree[question].append(true_case)\n",
    "            sub_tree[question].append(false_case)\n",
    "        \n",
    "        return sub_tree\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "#=================================================================================\n",
    "\n",
    "# use the tree to make prediction(classify) the instance\n",
    "# recursion until the instance reach to the class (not sub question)\n",
    "\n",
    "def classify_instance(instance, tree):\n",
    "    question = list(tree.keys())[0]\n",
    "    \n",
    "    feature_name, comparsion_operator, value = question.split()\n",
    "    \n",
    "    # in case continuous\n",
    "    if comparsion_operator == \"<=\":\n",
    "\n",
    "        if instance[feature_name] <= float(value):\n",
    "            ans = tree[question][0]\n",
    "        else:\n",
    "            ans = tree[question][1]\n",
    "    \n",
    "    # in case categorical\n",
    "    else:\n",
    "        if str(instance[feature_name]) == value:\n",
    "            ans = tree[question][0]\n",
    "        else:\n",
    "            ans = tree[question][1]\n",
    "    \n",
    "    # base case: check data type\n",
    "    if isinstance(ans,dict) == False:\n",
    "        return ans\n",
    "\n",
    "    # if did not reach the class = ans is dictionary(sub-tree)\n",
    "    else: \n",
    "        return classify_instance(instance, ans)\n",
    "\n",
    "#=================================================================================\n",
    "\n",
    "def calculate_accuracy(df, tree):\n",
    "    \n",
    "    # create column of predictions by applying the classify_instance function\n",
    "    # along an axis of the df\n",
    "    df[\"classification\"] = df.apply(classify_instance, \n",
    "                                    axis = 1,\n",
    "                                    args = (tree,))\n",
    "    \n",
    "    df[\"classification_result\"] = df.classification == df.label\n",
    "    \n",
    "    accuracy = df.classification_result.mean()\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "#=================================================================================\n",
    "\n",
    "def predict_example(example, tree):\n",
    "    question = list(tree.keys())[0]\n",
    "    feature_name, comparison_operator, value = question.split(\" \")\n",
    "\n",
    "    # ask question\n",
    "    if comparison_operator == \"<=\":\n",
    "        if example[feature_name] <= float(value):\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "    \n",
    "    # feature is categorical\n",
    "    else:\n",
    "        if str(example[feature_name]) == value:\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "\n",
    "    # base case\n",
    "    if not isinstance(answer, dict):\n",
    "        return answer\n",
    "    \n",
    "    # recursive part\n",
    "    else:\n",
    "        residual_tree = answer\n",
    "        return predict_example(example, residual_tree)\n",
    "\n",
    "#=================================================================================\n",
    "\n",
    "def decision_tree_predictions(test_df, tree):\n",
    "    predictions = test_df.apply(predict_example, args = (tree,), axis = 1)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "# Given with df and test_size(porportion) as arguments, split the\n",
    "# original df into two parts by randomly selecting indices \n",
    "\n",
    "def train_test_split(df, test_size):\n",
    "    test_size = int(len(df) * test_size)\n",
    "\n",
    "    # indices contain all the index values of the DF\n",
    "    indices = df.index.tolist()\n",
    "\n",
    "    # Chooses k unique random elements from a population sequence or set for test indices \n",
    "    test_indices = random.sample(population=indices, k = test_size)\n",
    "\n",
    "    # loc[] allows us to acces certain rows\n",
    "    test_df = df.loc[test_indices]\n",
    "\n",
    "    # Create test_df by dropping test_indices(rows) from original DF\n",
    "    train_df = df.drop(test_indices)\n",
    "\n",
    "    return train_df, test_df\n",
    "    \n",
    "\n",
    "\n",
    "# String data = categorical date\n",
    "def determine_type_of_feature(df):\n",
    "    data_types = [] \n",
    "   \n",
    "    # number to decide whether the feature is categorical or continous\n",
    "    n_unique_values_threshold = 15\n",
    "    \n",
    "    for column in df.columns:\n",
    "        unique_values = df[column].unique()\n",
    "        \n",
    "        #storing an example data to check whether it is a String type data\n",
    "        example_value = unique_values[0]\n",
    "        \n",
    "        if(isinstance(example_value,str)) or (len(unique_values) <= n_unique_values_threshold):\n",
    "            data_types.append(\"categorical\")\n",
    "        else:\n",
    "            data_types.append(\"continuous\")\n",
    "        \n",
    "    return data_types\n",
    "\n",
    "\n",
    "def calculate_accuracy(predictions, labels):\n",
    "    predictions_correct = predictions == labels\n",
    "    accuracy = predictions_correct.mean()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
